to start:
# docker-compose up
# docker exec -it ksqldb-cli ksql http://ksqldb-server:8088


QUESTION 01 :
ksql> CREATE TYPE season length AS STRUCT<season id INT, episode count INT>;
ksql> SHOW TYPES;

QUESTION 02 :
The appropriate collection type for titles and production changes :
• titles : tables, they are mutable.
• production changes : stream because they are immutable events.

QUESTION 03 :
ksql> CREATE STREAM productionChanges (
    rowkey VARCHAR KEY,title id INT, change type VARCHAR,
    before SEASON LENGTH,after SEASON LENGTH,created at VARCHAR
  )
  WITH (
    kafka topic=’production changes’,
    value format=’json’,
    partitions=4,
    TIMESTAMP=’created at’,
    TIMESTAMP FORMAT=’yyyy-MM-dd HH:mm:ss’
    Artificial Intelligence and Data Science
    1Big Data Technologies
  );
ksql> CREATE TABLE title (id INT PRIMARY KEY,title VARCHAR)
WITH (KAFKA TOPIC=’titles’,PARTITIONS=’4’,VALUE FORMAT=’JSON’);

QUESTION 04 :
ksql> INSERT INTO title (id, title) VALUES (1,’title1’);
ksql> INSERT INTO productionChanges (rowkey, title id,change type,before,after,created at)
  VALUES (’rowkey1’,1,’season length’,STRUCT(season id := 1,episode count := 12),
  STRUCT(season id := 1,episode count := 8),’2021-02-08 11:30:00’);

QUESTION 05 :
ksql> SET ’auto.offset.reset’ = ’earliest’;
ksql> SELECT * FROM productionChanges
  WHERE created at < ’2023-04-14 à 12:00:00’
  EMIT CHANGES;

QUESTION 06 :
ksql> SELECT * FROM productionChanges
  WHERE change type LIKE ’season%’
  EMIT CHANGES;

QUESTION 07 :
ksql> CREATE STREAM season length changes
  WITH (KAFKA TOPIC=’season length changes’,PARTITIONS=’4’,VALUE FORMAT=’JSON’,REPLICAS = ’1’)
  AS
  SELECT rowkey,title id,created at,
  IFNULL(after->season id, before->season id) as season id,
  before->episode count as old episode count,
  after->episode count as new episode count
  FROM productionChanges
  WHERE change type = ’season length’;

QUESTION 08 :
ksql> SELECT title
  FROM season length changes s
  INNER JOIN title t
  ON CAST(s.title id AS INT) = t.id
  EMIT CHANGES ;

QUESTION 09 :
ksql> CREATE STREAM season length changes enriched
  WITH (
    KAFKA TOPIC=’season length changes enriched’,
    PARTITIONS=’4’,
    VALUE FORMAT=’JSON’,
    REPLICAS = ’1’,
    TIMESTAMP=’created at’,
    TIMESTAMP FORMAT=’yyyy-MM-dd HH:mm:ss’
  ) AS
    SELECT rowkey,title id,created at,old episode count,new episode count,title,id
    FROM season length changes s
    INNER JOIN title t
    ON CAST(s.title id AS INT) = t.id;

QUESTION 10 :
ksql> CREATE TABLE season length change counts
  WITH (
    KAFKA TOPIC=’season length change counts’,
    PARTITIONS=’4’,
    VALUE FORMAT=’JSON’,
    REPLICAS = ’1’
  ) AS
    SELECT title id,COUNT(*) AS change count,LATEST BY OFFSET(new episode count) AS latest episode count
    FROM season length changes enriched
    WINDOW TUMBLING (SIZE 1 HOUR)
    GROUP BY title id;


EXO02: 


from pyspark.sql import SparkSession
from time import sleep
spark = SparkSession.builder.appName("Chapitre4").getOrCreate()
static = spark.read.json("./data")
dataSchema = static.schema
static.printSchema()
streaming = spark.readStream.schema(dataSchema).option("maxFilesPerTrigger", 1).json("./dat
activityCounts = streaming.groupBy("gt").count()
activityQuery = activityCounts.writeStream.queryName("activity_counts") \
  .format("memory").outputMode("complete") \
  .start()
spark.streams.active
for x in range(5):
  spark.sql("SELECT * FROM activity_counts").show()
  sleep(1)
